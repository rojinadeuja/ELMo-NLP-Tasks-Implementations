{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization & Feature Extraction using Keras' Tokenizer\n",
    "\n",
    "After the Data Preprocessing, we are ready to use Keras' Tokenizer class to tokenize and create numeric features based on the new text corpus that contains most common phrases.\n",
    "\n",
    "We will use the following Tokenizer methods:\n",
    "\n",
    "> fit_on_texts (Vectorize a text corpus, by turning each text into either a sequence of integerr. Each integer represents the index of a token in a dictionary)\n",
    "\n",
    "> texts_to_sequences (Transforms each training text in texts to a sequence of integers)\n",
    "\n",
    "> pad_sequences (Add padding to a text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>type</th>\n",
       "      <th>Processed_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>train</td>\n",
       "      <td>the rock is destined to be the 21st century ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>train</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "      <td>train</td>\n",
       "      <td>singer composer bryan adam contributes slew of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>train</td>\n",
       "      <td>you think by now america would have had enough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>train</td>\n",
       "      <td>yet the act is still charming here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                             review  \\\n",
       "0           0      4  The Rock is destined to be the 21st Century 's...   \n",
       "1           1      5  The gorgeously elaborate continuation of `` Th...   \n",
       "2           2      4  Singer/composer Bryan Adams contributes a slew...   \n",
       "3           3      3  You 'd think by now America would have had eno...   \n",
       "4           4      4               Yet the act is still charming here .   \n",
       "\n",
       "    type                                  Processed_Reviews  \n",
       "0  train  the rock is destined to be the 21st century ne...  \n",
       "1  train  the gorgeously elaborate continuation of the l...  \n",
       "2  train  singer composer bryan adam contributes slew of...  \n",
       "3  train  you think by now america would have had enough...  \n",
       "4  train                 yet the act is still charming here  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('Datasets/SST5_master_train.csv')\n",
    "df_train.Processed_Reviews = df_train.Processed_Reviews.astype(str)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>type</th>\n",
       "      <th>Processed_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9645</td>\n",
       "      <td>4</td>\n",
       "      <td>It 's a lovely film with lovely performances b...</td>\n",
       "      <td>test</td>\n",
       "      <td>it lovely film with lovely performance by buy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9646</td>\n",
       "      <td>3</td>\n",
       "      <td>No one goes unindicted here , which is probabl...</td>\n",
       "      <td>test</td>\n",
       "      <td>no one go unindicted here which is probably fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9647</td>\n",
       "      <td>4</td>\n",
       "      <td>And if you 're not nearly moved to tears by a ...</td>\n",
       "      <td>test</td>\n",
       "      <td>and if you re not nearly moved to tear by coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9648</td>\n",
       "      <td>5</td>\n",
       "      <td>A warm , funny , engaging film .</td>\n",
       "      <td>test</td>\n",
       "      <td>warm funny engaging film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9649</td>\n",
       "      <td>5</td>\n",
       "      <td>Uses sharp humor and insight into human nature...</td>\n",
       "      <td>test</td>\n",
       "      <td>us sharp humor and insight into human nature t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                             review  type  \\\n",
       "0        9645      4  It 's a lovely film with lovely performances b...  test   \n",
       "1        9646      3  No one goes unindicted here , which is probabl...  test   \n",
       "2        9647      4  And if you 're not nearly moved to tears by a ...  test   \n",
       "3        9648      5                   A warm , funny , engaging film .  test   \n",
       "4        9649      5  Uses sharp humor and insight into human nature...  test   \n",
       "\n",
       "                                   Processed_Reviews  \n",
       "0  it lovely film with lovely performance by buy ...  \n",
       "1  no one go unindicted here which is probably fo...  \n",
       "2  and if you re not nearly moved to tear by coup...  \n",
       "3                           warm funny engaging film  \n",
       "4  us sharp humor and insight into human nature t...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('Datasets/SST5_master_test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text\n",
    "\n",
    "Keras' Tokenizer class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 209 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Tokenizing the text\n",
    "\n",
    "- num_words: the maximum number of words to keep\n",
    "- oov_token: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n",
    "'''\n",
    "# We will keep only the top max_words number of words (high-frequency tokens) from the dataset.\n",
    "# This will be used to define the fixed length of the feature vectors.\n",
    "max_words = 100 \n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words, oov_token = '<OOV>')\n",
    "\n",
    "# Fit the Tokenizer object on the training data.\n",
    "# This updates internal vocabulary based on a list of tokenized texts.\n",
    "tokenizer.fit_on_texts(df_train['Processed_Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words (tokens): 14796\n",
      "\n",
      "Index of the word 'the': 2\n",
      "\n",
      "Size of vocabulary:  14797\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Full list of words are available through the \"word_index\" property of tokenizer.\n",
    "It returns a dictionary of key-value pairs, in which each word is a key,\n",
    "and its index is a value.\n",
    "\n",
    "'''\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Number of unique words (tokens): %d\" % len(word_index))\n",
    "\n",
    "# Print the index of the word \"the\"\n",
    "print(\"\\nIndex of the word 'the':\", word_index.get(\"the\"))\n",
    "\n",
    "vocab_size = len(word_index) + 1\n",
    "print(\"\\nSize of vocabulary: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Transforms each text in texts to a sequence of integers.\n",
    "'''\n",
    "\n",
    "# Transforms each training text in texts to a sequence of integers.\n",
    "sequences_train = tokenizer.texts_to_sequences(df_train['Processed_Reviews']) \n",
    "\n",
    "# Transforms each test text in texts to a sequence of integers.\n",
    "sequences_test = tokenizer.texts_to_sequences(df_test['Processed_Reviews']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the Padded Training Data Tensor:  (9645, 100)\n",
      "Shape of the Training Label Tensor:  (9645,)\n",
      "\n",
      "Shape of the Padded Test Data Tensor:  (1101, 100)\n",
      "Shape of the Test Label Tensor:  (1101,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Add padding to the beginning of a text (sentence).\n",
    "The number of padds is determined based on the length of the longest text.\n",
    "\n",
    "The \"pad_sequences\" function transforms a list (of length num_samples) of sequences (lists of integers) \n",
    "into a 2D Numpy array of shape (num_samples, num_timesteps). \n",
    "The num_timesteps is either the maxlen argument if provided, or the length of the longest sequence in the list.\n",
    "Sequences that are shorter than num_timesteps are padded with value until they are num_timesteps long.\n",
    "\n",
    "Arguments:\n",
    "- maxlen=None\n",
    "- dtype='int32'\n",
    "- padding='pre' (padding is added at the beginning)\n",
    "- truncating='pre' (if a sentence is longer than the \"maxlen\", then cut the sentence at the beginning)\n",
    "\n",
    "'''\n",
    "\n",
    "maxlen = 100 # Cuts off reviews after 100 words \n",
    "\n",
    "padded_data_train = pad_sequences(sequences_train, maxlen=maxlen)\n",
    "\n",
    "padded_data_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# Transform the labels as a numpy array\n",
    "labels_train = np.asarray(df_train['label'])\n",
    "labels_test = np.asarray(df_test['label'])\n",
    "\n",
    "# Show output array shapes\n",
    "print(\"\\nShape of the Padded Training Data Tensor: \", padded_data_train.shape)\n",
    "print(\"Shape of the Training Label Tensor: \", labels_train.shape)\n",
    "print(\"\\nShape of the Padded Test Data Tensor: \", padded_data_test.shape)\n",
    "print(\"Shape of the Test Label Tensor: \", labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data:  (9645, 100)\n",
      "Training Label:  (9645,)\n",
      "\n",
      "Test Data:  (1101, 100)\n",
      "Test Label:  (1101,)\n"
     ]
    }
   ],
   "source": [
    "X_train = padded_data_train \n",
    "y_train = labels_train\n",
    "\n",
    "print(\"\\nTraining Data: \", X_train.shape)\n",
    "print(\"Training Label: \", y_train.shape)\n",
    "\n",
    "X_test = padded_data_test \n",
    "y_test = labels_test\n",
    "\n",
    "print(\"\\nTest Data: \", X_test.shape)\n",
    "print(\"Test Label: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# url = \"https://tfhub.dev/google/elmo/2\"\n",
    "# embed = hub.Module(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfhub_dir = '/data/jupyter/common/model/text/tfhub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, os\n",
    "# def add_aion(curr_path=None):\n",
    "#     if curr_path is None:\n",
    "#         dir_path = os.getcwd()\n",
    "#         target_path = os.path.dirname(dir_path)\n",
    "#         if target_path not in sys.path:\n",
    "#             print('Added %s into sys.path.' % (target_path))\n",
    "#             sys.path.insert(0, target_path)\n",
    "            \n",
    "# add_aion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aion.embeddings.elmo import ELMoEmbeddings\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from keras import backend as K\n",
    "# from keras.layers import Input, Lambda, Dense, Embedding, BatchNormalization, Concatenate, LSTM\n",
    "# from keras.models import Model\n",
    "\n",
    "# elmo_embs = ELMoEmbeddings(layer='elmo', verbose=20)\n",
    "# elmo_embs.load(dest_dir=tfhub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input Layers\n",
    "# word_input_layer = Input(shape=(None, ), dtype='int32')\n",
    "# elmo_input_layer = Input(shape=(None, ), dtype=tf.string)\n",
    "\n",
    "# # Output Layers\n",
    "# word_output_layer = Embedding(input_dim=vocab_size, output_dim=256)(word_input_layer)\n",
    "\n",
    "# elmo_output_layer = Lambda(elmo_embs.to_keras_layer, output_shape=(None, 1024))(elmo_input_layer)\n",
    "\n",
    "# output_layer = Concatenate()([word_output_layer, elmo_output_layer])\n",
    "# output_layer = BatchNormalization()(output_layer)\n",
    "# output_layer = LSTM(256, dropout=0.2, recurrent_dropout=0.2)(output_layer)\n",
    "# output_layer = Dense(4, activation='sigmoid')(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The pretrained embedding vectors has length (dimension) 300.\n",
    "# embedding_dim = 300\n",
    "\n",
    "# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# pretrained_embeddings = 0\n",
    "\n",
    "# # Create a weight matrix for words in training docs\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = getVector(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "#         pretrained_embeddings +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of vocabulary words that are not present in the pre-trained dictionary: \", \n",
    "#       vocab_size - pretrained_embeddings )\n",
    "\n",
    "# print(\"\\nPercentage of pre-trained vectors used: %.2f\" % ((pretrained_embeddings*100.0)/vocab_size))\n",
    "\n",
    "# print(\"\\nWeight Matrix shape: \", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Classifier to Fine-tune & Learn Embeddings\n",
    "\n",
    "We train the classifier to fine-tune the pretrained embedding as well as to learn embeddings for the words that were not present in the pretrained dictionary.\n",
    "\n",
    "We need to set the Keras Embedding layer for the fine-tuning purpose.\n",
    "\n",
    "**Set the Embedding Layer using Pretrained Embeddings**\n",
    "\n",
    "\n",
    "Embedding layer has two mandatory arguments:\n",
    "- input_dim: vocab_size\n",
    "\n",
    "          -- The number of unique words in the input dataset. \n",
    "\n",
    "- output_dim: embedding_dim \n",
    "\n",
    "        -- The size of Embedding word vectors. For the pretrained Word2vec embeddings the embedding_dim is 300.\n",
    "\n",
    "\n",
    "To use pre-trained word vectors, we need to set two more parameters.\n",
    "- embedding_matrix: it is the weights parameter \n",
    "- trainable: it should be set to False to keep the embeddings fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine import Layer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create a custom layer that allows us to update weights (lambda layers do not have trainable parameters!)\n",
    "\n",
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 32\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['default']\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return K.not_equal(inputs, '--PAD--')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the TensorFlow graph before creating a new model, otherwise memory overflow will occur.\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Set random seed for reproducable results\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to build model\n",
    "def build_model(): \n",
    "    input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
    "    \n",
    "    embedding = ElmoEmbeddingLayer()(input_text)\n",
    "    \n",
    "    dense = layers.Dense(100, activation='relu')(embedding)\n",
    "    \n",
    "    pred = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=[input_text], outputs=pred)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create biLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False))\n",
    "# model.add(keras.layers.Bidirectional(keras.layers.LSTM(100)))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a path for the log sub-directory as curdir + Logs + currdatetime + modelname\n",
    "# model_name = \"Embedding-biLSTM\"\n",
    "# model_name_format = \"Embedding-Dense-biLSTM.h5\"\n",
    "# run_logdir = os.path.join(os.curdir, \"Logs\", time.strftime(\"run_%Y_%m_%d-%H_%M_%S\"), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-54c310e608bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Build and fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m model.fit(train_text, \n\u001b[0;32m      4\u001b[0m           \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-c58b2c99e67d>\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElmoEmbeddingLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m             \u001b[1;31m# If the layer returns tensors from its inputs, unmodified,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m             \u001b[1;31m# we copy them to avoid loss of tensor metadata.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[0moutput_ls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-ef0b9c9a1b9d>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     24\u001b[0m                                name=\"{}_module\".format(self.name))\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"^{}_module/.*\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mElmoEmbeddingLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'tf'"
     ]
    }
   ],
   "source": [
    "# Build and fit\n",
    "model = build_model()\n",
    "model.fit(train_text, \n",
    "          train_label,\n",
    "          validation_data=(test_text, test_label),\n",
    "          epochs=1,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "# epochs = 5\n",
    "\n",
    "# params={\n",
    "#     \"batch_size\":batch_size,\n",
    "#     \"epochs\":epochs\n",
    "# }\n",
    "\n",
    "# # experiment.log_parameters(params)\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# # with experiment.train():\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                         epochs = epochs,\n",
    "#                         batch_size = batch_size,\n",
    "#                         verbose = True,\n",
    "#                         validation_data = (X_val, y_val),\n",
    "#                         callbacks= [tensorboard_cb, loss_history_cb]) \n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# model.save(model_name_format)\n",
    "\n",
    "# duration_Pretraining_sec = t1-t0\n",
    "# duration_Pretraining = convertTime(t1 - t0)\n",
    "\n",
    "# print(\"\\nTraining Time: \", duration_Pretraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classifier on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfEpochs = 5\n",
    "print(\"Epochs: \", numOfEpochs)\n",
    "\n",
    "# It will log metrics with the prefix 'test_'\n",
    "test_loss_mlp, test_accuracy_mlp = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "metrics = {\n",
    "    'loss':test_loss_mlp,\n",
    "    'accuracy':test_accuracy_mlp\n",
    "}\n",
    "\n",
    "#experiment.log_metrics(metrics)\n",
    "\n",
    "print(\"\\nTest Accuracy: {:.3f}\".format(test_accuracy_mlp))\n",
    "print(\"Test Loss: {:.3f}\".format(test_loss_mlp))\n",
    "\n",
    "#y_test_predicted = (model.predict(X_test)>0.5)\n",
    "\n",
    "y_test_predicted_proba = model.predict(X_test)\n",
    "\n",
    "y_test_predicted = np.zeros((len(y_test),), dtype=int)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test_predicted_proba[i] > 0.5):\n",
    "        y_test_predicted[i] = 1\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_test_predicted[i]:\n",
    "        true += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test: Correct Predictions: {}\".format(true))\n",
    "print(\"Test: Incorrect Predictions: {}\".format(len(y_test_predicted) - true))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "# experiment.log_confusion_matrix(y_test.ravel(), y_test_predicted)\n",
    "\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, numOfEpochs, savePlot=False, plotName=None):\n",
    "    '''Function For Generating Learning Curves (Accuracy & Loss)'''\n",
    "    \n",
    "    plt.figure(figsize=(18,6))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(range(1,numOfEpochs+1),history.history['val_accuracy'],label='validation')\n",
    "    plt.plot(range(1,numOfEpochs+1),history.history['accuracy'],label='training')\n",
    "    plt.legend(loc=0)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim([1,numOfEpochs])\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(range(1,numOfEpochs+1),history.history['val_loss'],label='validation')\n",
    "    plt.plot(range(1,numOfEpochs+1),history.history['loss'],label='training')\n",
    "    plt.legend(loc=0)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlim([1,numOfEpochs])\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if(savePlot == True):\n",
    "        plt.savefig(plotName, dpi=300)\n",
    "\n",
    "    \n",
    "    plt.show() \n",
    "\n",
    "\n",
    "def plot_learning_rate(loss_history_lschedule, numOfEpochs, momentumPlot=False):\n",
    "    '''Function to plot learning rate and momentum'''\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(1,numOfEpochs+1),loss_history_lschedule.lr,label='learning rate')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xlim([1,numOfEpochs+1])\n",
    "    plt.ylabel(\"Learning rate\")\n",
    "    \n",
    "    if(momentumPlot==True):\n",
    "        plt.plot(range(1,numOfEpochs+1),loss_history_lschedule.mom,'r-', label='momentum')\n",
    "        plt.ylabel(\"Learning rate & Momentum\")\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history, numOfEpochs, savePlot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_rate(loss_history_cb, numOfEpochs,  momentumPlot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
